{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ec4ba3-d8ab-4964-8f9d-d86efeb0afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import os\n",
    "import os.path\n",
    "import shutil\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f1fa7-f27f-47a0-9ff0-ac1167457a17",
   "metadata": {},
   "source": [
    "# Ingest JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ce596-25d7-4e60-9d2a-3000c12a5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Current WD to json data\n",
    "if (os.getcwd()[-31:] != \"Messenger-Mining\\\\data\\\\live\\\\json\"):\n",
    "    os.chdir(\"../data/live/json\")\n",
    "# Store filenames of all files in the directory to files.\n",
    "files = []\n",
    "for path in os.listdir(os.getcwd()):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(os.getcwd(), path)):\n",
    "        files.append(path)\n",
    "\n",
    "participants = []\n",
    "for file_name in files:\n",
    "    with open(file_name,\"r\") as read_file:\n",
    "        data = json.load(read_file)\n",
    "        if len(data['participants']) <= 2:\n",
    "            participants.append(data['participants'])\n",
    "\n",
    "participants    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f617d631-6bf4-4c8d-af99-946f4abceabb",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd97963-2d96-43e7-9a6a-cd8dea3f5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('nps_chat')\n",
    "posts = nltk.corpus.nps_chat.xml_posts()\n",
    "\n",
    "\n",
    "posts_text = [post.text for post in posts]\n",
    "\n",
    "#divide train and test in 80 20\n",
    "train_text = posts_text[:int(len(posts_text)*0.8)]\n",
    "test_text = posts_text[int(len(posts_text)*0.2):]\n",
    "\n",
    "#Get TFIDF features\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3), \n",
    "                             min_df=0.001, \n",
    "                             max_df=0.7, \n",
    "                             analyzer='word')\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_text)\n",
    "X_test = vectorizer.transform(test_text)\n",
    "\n",
    "y = [post.get('class') for post in posts]\n",
    "\n",
    "y_train = y[:int(len(posts_text)*0.8)]\n",
    "y_test = y[int(len(posts_text)*0.2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e75ea51-de36-4a65-901c-21288621f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Gradient Boosting classifier to the Training set\n",
    "#Can be improved with Cross Validation\n",
    "gb = GradientBoostingClassifier(n_estimators = 400, random_state=0)\n",
    "gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ed499-bfa1-49d6-9828-a8c1969c945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rf = gb.predict(X_test)\n",
    "print(classification_report(y_test, predictions_rf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
